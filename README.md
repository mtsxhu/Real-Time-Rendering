# 第二章 · 图形渲染管线 The Graphics Rendering Pipeline

## 一、章节介绍及思维导图

&#x20;           本章介绍了实时图形的核心组件，图形渲染管线，也称为“管线”、“渲染流水线”等。管线的主要功能是通过给定的虚拟相机、三维物体、光源等条件，生成或渲染一个二维图像。因此，渲染管线是实时渲染的底层工具。

![](.gitbook/assets/Snipaste\_2021-09-20\_15-23-43.png)

## 二、核心内容

&#x20;            图2.1展示了渲染管线的使用步骤，图片中物体的位置和形状由它们的几何形状、环境特征、环境中摄像机的位置所决定。物体的外观受到由材质、光照、纹理和着色方程的影响。

![ 图2.1](.gitbook/assets/image.png)

&#x20;         在左图中，一个虚拟相机位于金字塔的右上角。只渲染视锥体（摄像机远近两个平面组成的几何形体）内部的物体。右图显示了相机“看到的”物体。注意，左边图像中的红色物体不在右边的渲染中，因为它位于视锥体的外部。同时，左边图像中的蓝色物体上面超出视锥体的部分也不在右边的渲染中。

&#x20;        图形渲染管线可以粗略的分为四个阶段：应用application、几何geometry、光栅化rasterization和像素pixel阶段。，如图2.2。网上很多资料也会将像素阶段放到光栅化阶段里，这里为了和原书同步，按四个阶段介绍。

![图2.2](.gitbook/assets/Snipaste\_2021-09-19\_08-36-52.png)

&#x20;        从图中可以看到，每一个阶段本身就是一个小的管线。之所以将渲染管线分为这么多个阶段，是为了可以并行化处理。整个流水线的速度（渲染速度），取决于流水线中最慢的那个阶段。可以用frames per second表示每秒绘制的图像数量，通常简称为FPS，也可以用Hz表示。

### 2.1 应用阶段 The Application Stage

&#x20;        应用阶段是由应用程序驱动的，通常在运行在通用cpu上的软件中实现，它不像几何处理、光栅化和像素处理阶段那样被划分为子阶段。为了提高性能，这一阶段通常在多个处理器核心上并行执行，在CPU设计中，这被称为超标量结构，因为它能够在同一阶段同时处理多个任务。

&#x20;        应用阶段通常会执行以下任务：碰撞检测、全局加速算法、动画、物理模拟等，这取决于应用程序的类型。应用阶段也是处理来自其他来源(如键盘、鼠标或头戴显示器)的输入，以及其他管道的其他部分无法处理的算法。

&#x20;        应用程序阶段最后会将渲染图元（点、线、三角形等）输入到渲染管线的下一个阶段，也就是几何阶段。

### 2.2 几何阶段 The Geometry Stage

&#x20;       GPU上的几何处理阶段负责大部分的三角形和顶点操作。这个阶段进一步分为以下功能阶段:顶点着色、投影、剪辑和屏幕映射，如图2.3。

![图2.3](.gitbook/assets/Snipaste\_2021-09-19\_09-23-24.png)

#### 2.2.1 顶点着色 Vertex Shading

&#x20;          顶点着色主要有两个任务，即顶点变换，以及计算顶点的材质数据（法线、纹理坐标），在屏幕上显示的过程中，模型被转换成几个不同的空间或坐标系统。模型坐标应用模型变换后位于世界坐标或世界空间中。世界空间是唯一的，模型经过各自的模型转换后，所有的模型都存在于同一个空间中。摄像机在世界坐标中有一个位置和方向，用于放置和校准相机，只有被摄像机看到的模型才会被显示在屏幕上。

&#x20;       为了方便投影和裁剪，摄像机和所有的模型都通过视图变换进行变换。视图变换的目的是将摄像机放置在原点并校准它，使它朝向负z轴方向（也可以朝向正z轴方向），y轴指向上方，x轴指向右方。经过试图变换后的模型位于视图空间或摄像机空间。图2.4展示了视图变换对相机和模型的影响。

![图2.4](.gitbook/assets/Snipaste\_2021-09-19\_10-56-41.png)

&#x20;       在左图中，自顶向下的视图显示了相机的位置和方向，就像用户希望的那样，在这个世界中+z轴是向上的。视图变换重新定位世界，使摄像机处于原点，沿着负z轴观察，摄像机的+y轴向上，如右图所示。这样做是为了使剪切和投影操作更简单和更快。 浅蓝色区域是视图体积。这里假设为透视视图，因为视图体积是一个截锥。类似的技术适用于任何类型的投影。

&#x20;       因此顶点变换主要包含模型变换以及视图变换，这两个变换都是为了建模和渲染更方便。为了产生一个真实的场景，仅仅渲染物体的形状和位置是不够的，它们的外观也必须建模。物体的材质，光照效果。材质和光照效果可以以任何方式建模，从简单的颜色描述到复杂的物理描述。

&#x20;     **** 确定光照对材质的影响的操作称为着色（shading），它涉及到计算物体上不同点的着色方程。通常，其中某些计算是在几何阶段中进行的，其他计算可能是在逐像素处理过程中执行的。材质数据可以存储在每个顶点上，如点的位置、法线、颜色或任何其他计算着色方程所需的数值信息。

&#x20;       顶点着色用于模型顶点变换以及确定顶点的光照效果。

#### 2.2.2 投影 Projection

&#x20;         顶点着色之后，渲染系统就开始执行投影操作，将视锥体转换为一个单位立方体，其顶点为 (−1,−1,−1)和(1,1,1)。同一体积可以使用不同的范围，如0≤z≤1。单位多维数据集称为规范视图体（Canonical View Volume，CVV）。常见的投影方法有两种：正交投影（平行投影）和透视投影。两种投影的效果如图2.5所示，左图为正交投影，右图为透视投影。

![图2.5](.gitbook/assets/Snipaste\_2021-09-19\_11-33-18.png)

&#x20;          正交投影的视图体积通常是一个矩形框，通常将其转换为单元立方体。正交投影的主要特点是平行线在变换后仍然保持平行。这种转换是平移和缩放的结合。

&#x20;          透视投影稍微复杂一点。在透视投影中，物体离相机越远，投影后就会显得越小。 并且平行线可能会相交于地平处。因此，透视变换模仿了人体感知物体大小的方式。

&#x20;           在任意一种投影变换之后，模型都被称为在裁剪坐标中，也可以叫做齐次裁剪坐标。虽然这些矩阵将一个体积转换成另一个体积，但它们被仍称为投影，是因为在显示之后，z坐标不会存储在生成的图像中，而是存储在z缓冲区中。

&#x20;           投影其实就是一个降维的过程，即将模型从三维空间转换到二维空间中。但实际上这个过程不会进行降维操作，仅仅计算出合适的用于投影的值，也就是说投影输出到下一阶段的数据仍然是齐次坐标。

#### 2.2.3 可选的顶点处理

&#x20;       第一个可选的阶段是曲面细分tessellation着色器，想象一个弹跳的球，如果使用一组三角形来表示它，则可能会遇到质量或性能方面的问题。这个球从远距离看可能还不错，但近距离看，个别的三角形，特别是沿轮廓的三角形，就会清晰可见。如果用更多的三角形来提高球的质量，可能会浪费大量的处理时间和内存，当球很远时，只覆盖屏幕上的几个像素，但要耗费更多的时间去渲染它。通过镶嵌，可以用适当数量的三角形生成曲面。

&#x20;       顶点可以用来描述一个曲面，比如一个球。这样的曲面可以由一组patch指定，每个patch由一组顶点组成。曲面细分阶段包括一系列的阶段本身外壳着色器，曲面细分器，和图元着色器，将这些patch顶点集转换成(通常)更大的顶点集，然后用来制作新的三角形集。场景中的摄像机可以用来确定生成了多少个三角形:当patch距离近时，生成的三角形很多，当patch距离较远时，生成的三角形很少。

&#x20;       下一个可选阶段是几何geometry着色器。这个着色器比曲面细分着色器更早，所以在GPU上更常见。它就像曲面细分着色器，它获取各种各样的图元，并可以产生新的顶点。这是一个简单得多的阶段，因为创建的范围有限，输出图元的类型也非常有限。几何着色器有几种用途，其中最流行的是粒子生成。想象一下模拟烟花爆炸。每个火球都可以用一个点来表示，一个单一的顶点。几何着色器可以把每个点变成一个正方形(由两个三角形组成)，面向观众，并覆盖几个像素，因此提供了一个更有说服力的原始颜色。

&#x20;       最后一个可选阶段称为流stream 输出。这个阶段让我们使用GPU作为几何引擎。此时，我们可以选择性地将这些处理过的顶点输出到数组（CPU）中以供进一步处理，而不是将处理过的顶点发送到渲染管线的其余部分以呈现到屏幕上。这些数据可以被CPU或GPU本身在以后的过程中使用。这个阶段通常用于粒子模拟，例如我们的焰火示例。

&#x20;        这三个阶段的执行顺序为：曲面细分、几何着色和流输出，每个阶段都是可选的。

#### 2.2.4 裁剪 Clipping

&#x20;       只有全部或部分在视锥体中的图元需要被传递到光栅化阶段(以及随后的像素处理阶段)，然后在屏幕上绘制它们。完全位于视锥体内部的图元将按原样传递到下一阶段。完全在视锥体之外的图元不会被进一步传递，因为它们不会被渲染。需要裁剪的是部分位于视锥体外的图元。例如，一个顶点在视锥体外，一个顶点在视图体积内的线应该被裁剪。使用投影矩阵意味着转换后的图元被裁剪到单位立方体上。在裁剪之前执行视图转换和投影的优点是它使裁剪问题保持一致;图元总是被裁剪到单元立方体上。

&#x20;       裁剪过程如图2.6所示。除了视锥体的6个裁剪平面外，用户还可以定义额外的裁剪平面进行裁剪。裁剪过程使用由投影产生的四值齐次坐标来执行裁剪。

![图2.6](.gitbook/assets/Snipaste\_2021-09-19\_15-24-07.png)

&#x20;       在投影变换之后，只需要对单位立方体内的图元(对应于视锥体内的图元)进行继续处理。因此，单位立方体外部的原语将被丢弃，而完全处于内部的图元将被保留。与单位立方体相交的图元被裁剪到单位立方体上，从而生成新的顶点并丢弃旧的顶点。

#### 2.2.5 屏幕映射 Screening Mapping

&#x20;      只有视锥体内的图元被传递到屏幕映射阶段，当进入这个阶段时，坐标仍然是三维的。每个图元的x和y坐标被转换成屏幕坐标。屏幕坐标和z坐标也称为窗口坐标。假设场景应该被渲染成一个窗口，最小的角在(x1, y1)，最大的角在(x2, y2)，其中x1 \<X2和y1 \<y2。然后屏幕映射是一个转换，然后是缩放操作。新的x和y坐标被称为屏幕坐标。z坐标(OpenGL的\[1，+1]和DirectX的\[0,1])也被映射到\[z1, z2]， z1 = 0和z2 = 1是默认值。但是，这些可以通过API进行更改。窗口坐标和重新映射的z值被传递到光栅化阶段。屏幕映射过程如图2.7所示。

![图2.7](.gitbook/assets/Snipaste\_2021-09-19\_15-50-39.png)

&#x20;        图元在投影变换后位于单元立方体中，屏幕映射过程负责在屏幕上寻找坐标。

### 2.3 光栅化阶段  Rasterization Stage

&#x20;        接收几何阶段传递来的经过变换和投影之后的顶点及其相关的颜色以及纹理坐标。光栅化的目标是找到原始图(例如一个三角形)中所有的像素的颜色。它被分为两个功能子阶段:三角形设置(也称为原始装配)和三角形遍历。需要注意的是光栅化阶段不仅仅只能处理三角形，也可以处理点和线。如图2.8所示。

![图2.8](.gitbook/assets/Snipaste\_2021-09-19\_16-13-58.png)

&#x20;       左:光栅化分为两个功能阶段，称为三角形设置和三角形遍历。右:像素处理分为像素处理和融合两个功能阶段。

&#x20;       三角形是否被认为是在像素内取决于你如何设置GPU的管道。例如，您可以使用点抽样来确定。最简单的情况是在每个像素的中心使用一个单点样本，因此如果中心点在三角形内，那么相应的像素也被认为在三角形内。您也可以使用超采样或多次采样反锯齿技术，每个像素使用多个样本。 另一种方法是使用保守的光栅化，其定义是，如果一个像素至少有一部分与三角形重叠，那么这个像素就在三角形的“内部”。

#### 2.3.1 三角形设置 Triangle Setup

&#x20;        对顶点的输入数据(比如，颜色、法线、纹理坐标)进行插值，得到各个片段对应的数据值。此任务在固定的硬件上执行。

#### 2.3.4 三角形遍历 Triangle Traversal

&#x20;          在这一阶段，每个像素的中心(或采样点)被三角形覆盖，在被三角形重叠的像素部分生成片段。找出哪些采样点或像素位于三角形内通常称为三角形遍历。每个三角形片段的属性都是通过在三个三角形顶点的数据插值生成的。这些属性包括片段的深度，以及任何来自几何阶段的着色数据。

### 2.4  像素处理阶段 Pixel Processing Stage

&#x20;       像素处理阶段分为像素着色和合并，如图2.8所示 。像素处理是对图元内部的像素或采样点执行逐像素或逐采样点计算和操作的阶段。

#### 2.4.1 像素着色 Pixel Shading

&#x20;          任何逐像素着色的计算都在这个阶段执行，使用插值得到的着色数据作为输入。生成多个颜色被传递到下一个阶段。不同于三角形设置和遍历阶段，像素着色阶段是在可编程GPU内执行的。因此，程序员可以为像素着色器(或片段着色器提供一个程序，它可以包含任何所需的计算。这里可以使用各种各样的技术，其中最重要的是纹理，纹理一个对象意味着将一个或多个图像“贴”在该对象上。这个过程的一个简单示例如图2.9所示。图像可能是一，二，或三维，其中二维图像是最常见的。

![图2.8](.gitbook/assets/Snipaste\_2021-09-19\_22-32-48.png)

&#x20;           左上角显示了一个没有纹理的龙模型，左下角显示的是贴上图像纹理的龙图形，右图为龙模型的纹理。

#### 2.4.2 合并 Merging

&#x20;        每个像素的信息存储在颜色缓冲区中，颜色缓冲区是由颜色组成的矩形数组(每种颜色有由红、绿和蓝色三个分量)。合并阶段负责将像素着色阶段产生的片段颜色与当前存储在缓冲区中的颜色合并。这个阶段也被称为ROP，代表“光栅操作(管线)”或“渲染输出单元”。与着色阶段不同，执行此阶段的GPU子单元通常不是完全可编程的。但是，它是高度可配置的，支持各种效果。

&#x20;        这个阶段还负责解决可见性问题。这意味着当整个场景被渲染时，颜色缓冲应该包含场景中从相机角度可见的图元的颜色。对于大多数甚至所有的图形硬件，这是通过z缓冲区(也称为深度缓冲区)来实现的。z缓冲区的大小和形状与颜色缓冲区相同，对于每个像素，它存储当前最接近摄像机的图元的z值。这意味着当一个图元被渲染到某个像素时，该图元在该像素处的z值将被计算并与相同像素处的z缓冲区的内容进行比较。如果新的z值小于z缓冲区中的z值，那么正在渲染的图元将比之前在该像素处最接近摄像机的图元更接近摄像机。因此，该像素的z值和颜色将更新为所绘制的图元的z值和颜色。如果计算的z值大于z缓冲区中的z值，则颜色缓冲区和z缓冲区将保持不变。

&#x20;       z-缓冲区算法很简单，有O(n)复杂度(n是被渲染的像素数量)，只要对每个图元计算出相应的像素z值，就可以使用这种方法。还要注意，该算法允许以任何顺序呈现大多数图元，这是它流行的另一个原因。然而，z-buffer仅在屏幕上的每个点存储单个深度，因此它不能用于半透明的图元。这些元素必须在所有不透明图元之后呈现，并且按照前后顺序呈现，或者使用独立于顺序的算法。透明度是基本z缓冲区的主要弱点之一。

　　颜色缓冲区用于存储颜色，z缓冲区用于存储每个像素的深度值。然而，还有其他通道和缓冲区可以用来过滤和捕获片段信息。alpha通道与颜色缓冲相关联，并为每个像素存储相关的不透明度值。alpha测试可在深度测试执行前在传入片段上运行。片段的alpha值与参考值作某些特定的测试，如果片段未能通过测试，它将不再进行进一步的处理。alpha测试经常用于不影响深度缓存的全透明片段的处理。

　　模板缓冲区是一个屏幕外缓冲区，用于记录所呈现图元的位置。它通常包含每像素8位。图元可以使用各种方法呈现到模板缓冲区中，然后可以使用缓冲区的内容控制颜色缓冲和z缓冲的渲染。例如，假设一个填充的圆已经绘制到模板缓冲区中。这可以与一个操作符结合使用，该操作符允许仅在有圆圈的地方将后续图元呈现到颜色缓冲区中。模板缓冲区是生成一些特殊效果的强大工具。管线末尾的所有这些方法都称为光栅操作 (ROP)或混合操作。可以将颜色缓冲中当前的颜色与三角形中正在处理的像素的颜色混合。这可以实现诸如透明度或颜色样本积累等效果。

　　帧缓冲区通常由系统上的所有缓冲区组成。通常是ｚ缓冲区和颜色缓冲区的集合。

　　图元达到并通过光栅化阶段时，那些从相机角度可见的图元就可以显示在屏幕上。屏幕显示颜色缓冲的内容。为了避免让人眼看到被栅格化并发送到屏幕的图元，使用了双重缓冲。这意味着场景的呈现发生在屏幕之外的后台缓冲区中。 在后台缓冲区中渲染场景后，后台缓冲区的内容将与之前显示在屏幕上的前台缓冲区的内容交换。只有当不影响显示的时候，才进行交换。

　

　
